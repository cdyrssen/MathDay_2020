In computer vision, pose estimation is the process of detecting a subject and mapping a virtual skeleton to key points of the subject representing joints. This has many applications from human input methods for virtual and augmented realities to control loops used in robotics. For this project a deep learning neural network model is proposed to utilize pose estimation and facial expression classifications to act as a virtual American Sign Language (ASL) interpreter. The goal of this network is to utilize existing datasets to convert an RGB image sequence or video of a subject communicating in ASL to English text. To accomplish this, existing datasets provide RGB data as well as at least one of the following features: depth data, 3D hand key points, or 2D hand key points. To develop a robust solution to this problem, the proposed model will utilizes both 2D keypoint predictor and a depth map predictor to produce a 3D pose from standard RGB data. The idea is the model learns to predict two features, namely 2D key points and depth data, which are then used to create a more accurate 3D pose estimation than just directly learning 3D pose key points from standard RGB data.
